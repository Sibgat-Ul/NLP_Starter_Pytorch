{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.755209737Z",
     "start_time": "2023-12-04T13:11:44.337736007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, IterableDataset\n",
    "\n",
    "import torchdata\n",
    "import torchtext\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "def unicodeToAscii(word):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', word)\n",
    "        if unicodedata.category(c) != 'Mn' and c in all_letters\n",
    "    )\n",
    "\n",
    "def readLine(fname):\n",
    "    file_name = re.findall(r'/data/names/(.*).txt', fname)[0]\n",
    "    with open(fname,encoding='utf-8') as file:\n",
    "        return file_name,[unicodeToAscii(line.strip()) for line in file]\n",
    "    \n",
    "def getCategory(path):\n",
    "    files = get_files(path)\n",
    "    category = [re.findall(r'/data/names/(.*).txt', file_name)[0] for file_name in files]\n",
    "    return category\n",
    "\n",
    "def getCatToName(path):\n",
    "    files = get_files(path)\n",
    "    cats = []\n",
    "    catToName = {}\n",
    "    for file in files:\n",
    "        cat, name = readLine(file)\n",
    "        cats.append(cat)\n",
    "        catToName[cat] = name\n",
    "    return cats, catToName"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.755546295Z",
     "start_time": "2023-12-04T13:11:44.682413684Z"
    }
   },
   "id": "1e370b21a37849b5"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \".,';-\"\n",
    "n_letters = len(all_letters) + 1 #for EOS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.755671404Z",
     "start_time": "2023-12-04T13:11:44.682558994Z"
    }
   },
   "id": "c30433579c1bb65"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "### Label\n",
    "path = './data/names/*.txt'\n",
    "### Lang to Name\n",
    "all_cat, catToLine = getCatToName(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.755991312Z",
     "start_time": "2023-12-04T13:11:44.682648633Z"
    }
   },
   "id": "df8b9220dbfff349"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class RNN_GEN_NAME(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_categories):\n",
    "        super(RNN_GEN_NAME, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.inputToHidden = nn.Linear(input_size+hidden_size+n_categories, hidden_size)\n",
    "        self.inputToOutput = nn.Linear(input_size+hidden_size+n_categories, output_size)\n",
    "        self.outToOut = nn.Linear(hidden_size+output_size, output_size)\n",
    "        \n",
    "        self.dropOut = nn.Dropout(0.3)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.softMax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, category, inp, hid):\n",
    "        input_comb = torch.cat((category,inp,hid), 1)\n",
    "        \n",
    "        hidden = self.inputToHidden(input_comb)\n",
    "        output = self.inputToOutput(input_comb)\n",
    "        \n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        \n",
    "        output = self.outToOut(output_combined)\n",
    "        output = self.dropOut(output)\n",
    "        output = self.softMax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros((1, self.hidden_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.756304140Z",
     "start_time": "2023-12-04T13:11:44.694802785Z"
    }
   },
   "id": "22991069e01dc956"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "def randomChoice(listToChoice):\n",
    "    return listToChoice[random.randint(0, len(listToChoice)-1)]\n",
    "\n",
    "def randomPair(catList, catToName):\n",
    "    randomCat = randomChoice(catList)\n",
    "    lineList = catToName[randomCat]\n",
    "    randomLine = randomChoice(lineList)\n",
    "    return randomCat, randomLine\n",
    "\n",
    "def randomExample(catList, oheCat, catToName):\n",
    "    cat, line = randomPair(catList, catToName)\n",
    "    catTensor = oheCat[cat]\n",
    "    lineTensor = lineToTensor(line)\n",
    "    return cat, line, catTensor, lineTensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.756509669Z",
     "start_time": "2023-12-04T13:11:44.697520877Z"
    }
   },
   "id": "c6bb622648a7fb37"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def getIndex(listToSearch, toSearch):\n",
    "    index = math.inf\n",
    "    lenList = len(listToSearch)\n",
    "\n",
    "    for i in range(lenList):\n",
    "        if listToSearch[i] == toSearch:\n",
    "            index = i\n",
    "            break\n",
    "\n",
    "    if index >= 0:\n",
    "        return index\n",
    "\n",
    "def lineToTensor(line, letters=all_letters):\n",
    "    tensor = torch.zeros([len(line), len(letters)])\n",
    "    for i, l in enumerate(line):\n",
    "        l_to_index = getIndex(letters, l)\n",
    "        tensor[i][l_to_index] = 1\n",
    "    return tensor\n",
    "\n",
    "def targetLineTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.756708878Z",
     "start_time": "2023-12-04T13:11:44.707183256Z"
    }
   },
   "id": "859428e614604b8a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "ex = lineToTensor('abd')\n",
    "print(torch.unsqueeze(ex, 1).size())\n",
    "ex = targetLineTensor('abd')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.862090271Z",
     "start_time": "2023-12-04T13:11:44.716675654Z"
    }
   },
   "id": "be91452cc9cfe54f"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def ohe_cat(cat):\n",
    "    lenCat = len(cat)\n",
    "    tensor = torch.zeros([lenCat, lenCat])\n",
    "    catToTensor = {}\n",
    "    for i in range(lenCat):\n",
    "        tensor[i][i] = 1\n",
    "        catToTensor[cat[i]] = tensor[i]\n",
    "    return catToTensor, tensor\n",
    "    \n",
    "def getIndex(listToSearch, toSearch):\n",
    "    index = math.inf\n",
    "    lenList = len(listToSearch)\n",
    "    \n",
    "    for i in range(lenList):\n",
    "        if listToSearch[i] == toSearch:\n",
    "            index = i\n",
    "            break\n",
    "    \n",
    "    if index >= 0:\n",
    "        return index "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.862298240Z",
     "start_time": "2023-12-04T13:11:44.757509532Z"
    }
   },
   "id": "744730a57945d00a"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "ohe_cat_mapped_tensor, oheCatTensor = ohe_cat(all_cat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.862464729Z",
     "start_time": "2023-12-04T13:11:44.757653512Z"
    }
   },
   "id": "58f73876d5e24d76"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.862531548Z",
     "start_time": "2023-12-04T13:11:44.757735881Z"
    }
   },
   "id": "9c7ef8d3151d4476"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "rnn = RNN_GEN_NAME(n_letters, 512, n_letters, len(all_cat))\n",
    "lossFunc = nn.NLLLoss()\n",
    "lr = 0.05\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def train(category_tensor, line_tensor, target_tensor):\n",
    "    target_tensor = torch.unsqueeze(target_tensor, -1)\n",
    "    line_tensor = torch.unsqueeze(line_tensor, 1)\n",
    "    \n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    loss = torch.Tensor([0])\n",
    "    \n",
    "    output = None\n",
    "    \n",
    "    for i in range(line_tensor.shape[0]):\n",
    "        print(category_tensor.shape, line_tensor.shape, hidden.shape)\n",
    "        output, hidden = rnn(category_tensor, line_tensor, hidden)\n",
    "        l = lossFunc(output, target_tensor[i])\n",
    "        loss += l\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-lr)\n",
    "\n",
    "    return output, loss.item() / line_tensor.size(0)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.862595928Z",
     "start_time": "2023-12-04T13:11:44.757809891Z"
    }
   },
   "id": "c4f205e07f8c02b"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18]) torch.Size([3, 1, 57]) torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m cat, line, cat_tensor, line_tensor \u001B[38;5;241m=\u001B[39m randomExample(all_cat, ohe_cat_mapped_tensor, catToLine)\n\u001B[1;32m     11\u001B[0m target_tensor \u001B[38;5;241m=\u001B[39m targetLineTensor(line)\n\u001B[0;32m---> 12\u001B[0m output, loss \u001B[38;5;241m=\u001B[39m train(cat_tensor, line_tensor, target_tensor)\n\u001B[1;32m     13\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m%\u001B[39m print_every \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[34], line 27\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(category_tensor, line_tensor, target_tensor)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(line_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mprint\u001B[39m(category_tensor\u001B[38;5;241m.\u001B[39mshape, line_tensor\u001B[38;5;241m.\u001B[39mshape, hidden\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m---> 27\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m rnn(category_tensor, line_tensor, hidden)\n\u001B[1;32m     28\u001B[0m     l \u001B[38;5;241m=\u001B[39m lossFunc(output, target_tensor[i])\n\u001B[1;32m     29\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m l\n",
      "File \u001B[0;32m~/anaconda3/envs/pyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/pyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[28], line 20\u001B[0m, in \u001B[0;36mRNN_GEN_NAME.forward\u001B[0;34m(self, category, inp, hid)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, category, inp, hid):\n\u001B[0;32m---> 20\u001B[0m     input_comb \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((category,inp,hid), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     22\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputToHidden(input_comb)\n\u001B[1;32m     23\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputToOutput(input_comb)\n",
      "\u001B[0;31mIndexError\u001B[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "n_iters = 100000\n",
    "print_every = 1000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every ``plot_every`` ``iters``\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    cat, line, cat_tensor, line_tensor = randomExample(all_cat, ohe_cat_mapped_tensor, catToLine)\n",
    "    target_tensor = targetLineTensor(line)\n",
    "    output, loss = train(cat_tensor, line_tensor, target_tensor)\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:11:44.917891343Z",
     "start_time": "2023-12-04T13:11:44.757891040Z"
    }
   },
   "id": "fc73b01da2fb62fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    with open(filename, encoding='utf-8') as some_file:\n",
    "        return [unicodeToAscii(line.strip()) for line in some_file]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "                       'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "                       'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.835456862Z"
    }
   },
   "id": "a26510cabef00800"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.device('cuda')\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        with torch.device('cuda'):\n",
    "            super(RNN, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "    \n",
    "            self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "            self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "            self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        with torch.device('cuda'):\n",
    "            input_combined = torch.cat((category, input, hidden), 1)\n",
    "            hidden = self.i2h(input_combined)\n",
    "            output = self.i2o(input_combined)\n",
    "            output_combined = torch.cat((hidden, output), 1)\n",
    "            output = self.o2o(output_combined)\n",
    "            output = self.dropout(output)\n",
    "            output = self.softmax(output)\n",
    "            return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        with torch.device('cuda'):\n",
    "            return torch.zeros(1, self.hidden_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.837913286Z"
    }
   },
   "id": "14d6beda9991e71d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    with torch.device('cuda'):\n",
    "        li = all_categories.index(category)\n",
    "        tensor = torch.zeros(1, n_categories, device='cuda')\n",
    "        tensor[0][li] = 1\n",
    "        return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    with torch.device('cuda'):\n",
    "        tensor = torch.zeros(len(line), 1, n_letters, device='cuda')\n",
    "        for li in range(len(line)):\n",
    "            letter = line[li]\n",
    "            tensor[li][0][all_letters.find(letter)] = 1\n",
    "        return tensor\n",
    "\n",
    "# ``LongTensor`` of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    with torch.device('cuda'):\n",
    "        letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "        letter_indexes.append(n_letters - 1) # EOS\n",
    "        return torch.LongTensor(letter_indexes).to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.840537929Z"
    }
   },
   "id": "eaa636af49802498"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make category, input, and target tensors from a random category, line pair\n",
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    with torch.device('cuda'):\n",
    "        category = randomChoice(all_categories)\n",
    "        line = randomChoice(category_lines[category])\n",
    "        return category, line\n",
    "\n",
    "def randomTrainingExample():\n",
    "    with torch.device('cuda'):\n",
    "        category, line = randomTrainingPair()\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input_line_tensor = inputTensor(line)\n",
    "        target_line_tensor = targetTensor(line)\n",
    "        return category_tensor, input_line_tensor, target_line_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.842552746Z"
    }
   },
   "id": "243f3ae1e29719b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss().to('cuda')\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    with torch.device('cuda'):\n",
    "        target_line_tensor.unsqueeze_(-1).to('cuda')\n",
    "        hidden = rnn.initHidden()\n",
    "    \n",
    "        rnn.zero_grad()\n",
    "    \n",
    "        loss = torch.Tensor([0]).to('cuda') # you can also just simply use ``loss = 0``\n",
    "    \n",
    "        for i in range(input_line_tensor.size(0)):\n",
    "            output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "            l = criterion(output, target_line_tensor[i]).to('cuda')\n",
    "            loss += l\n",
    "    \n",
    "        loss.backward()\n",
    "    \n",
    "        for p in rnn.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    \n",
    "        return output, loss.item() / input_line_tensor.size(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.844456004Z"
    }
   },
   "id": "8aaf46cf2a360a67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.846126573Z"
    }
   },
   "id": "abae27b0e3edd0f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters).to('cuda')\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 500\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every ``plot_every`` ``iters``\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    with torch.device('cuda'):\n",
    "        output, loss = train(*randomTrainingExample())\n",
    "        total_loss += loss\n",
    "    \n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "    \n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.887353089Z"
    }
   },
   "id": "6e91351f69dca77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.887462058Z"
    }
   },
   "id": "45f753dbdb9eadfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.887523728Z"
    }
   },
   "id": "9cd0fb3476196946"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m samples(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mArabic\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQSA\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[22], line 28\u001B[0m, in \u001B[0;36msamples\u001B[0;34m(category, start_letters)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msamples\u001B[39m(category, start_letters\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mABC\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m start_letter \u001B[38;5;129;01min\u001B[39;00m start_letters:\n\u001B[0;32m---> 28\u001B[0m         \u001B[38;5;28mprint\u001B[39m(sample(category, start_letter))\n",
      "Cell \u001B[0;32mIn[22], line 13\u001B[0m, in \u001B[0;36msample\u001B[0;34m(category, start_letter)\u001B[0m\n\u001B[1;32m     10\u001B[0m output_name \u001B[38;5;241m=\u001B[39m start_letter\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_length):\n\u001B[0;32m---> 13\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m rnn(category_tensor, \u001B[38;5;28minput\u001B[39m[\u001B[38;5;241m0\u001B[39m], hidden)\n\u001B[1;32m     14\u001B[0m     topv, topi \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mtopk(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     15\u001B[0m     topi \u001B[38;5;241m=\u001B[39m topi[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/pyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/pyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[28], line 20\u001B[0m, in \u001B[0;36mRNN_GEN_NAME.forward\u001B[0;34m(self, category, inp, hid)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, category, inp, hid):\n\u001B[0;32m---> 20\u001B[0m     input_comb \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((category,inp,hid), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     22\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputToHidden(input_comb)\n\u001B[1;32m     23\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputToOutput(input_comb)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "samples('Arabic', 'QSA')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:12:33.750840693Z",
     "start_time": "2023-12-04T13:12:33.672993739Z"
    }
   },
   "id": "414d96421b27a359"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "['Khoury',\n 'Nahas',\n 'Daher',\n 'Gerges',\n 'Nazari',\n 'Maalouf',\n 'Gerges',\n 'Naifeh',\n 'Guirguis',\n 'Baba',\n 'Sabbagh',\n 'Attia',\n 'Tahan',\n 'Haddad',\n 'Aswad',\n 'Najjar',\n 'Dagher',\n 'Maloof',\n 'Isa',\n 'Asghar',\n 'Nader',\n 'Gaber',\n 'Abboud',\n 'Maalouf',\n 'Zogby',\n 'Srour',\n 'Bahar',\n 'Mustafa',\n 'Hanania',\n 'Daher',\n 'Tuma',\n 'Nahas',\n 'Saliba',\n 'Shamoon',\n 'Handal',\n 'Baba',\n 'Amari',\n 'Bahar',\n 'Atiyeh',\n 'Said',\n 'Khouri',\n 'Tahan',\n 'Baba',\n 'Mustafa',\n 'Guirguis',\n 'Sleiman',\n 'Seif',\n 'Dagher',\n 'Bahar',\n 'Gaber',\n 'Harb',\n 'Seif',\n 'Asker',\n 'Nader',\n 'Antar',\n 'Awad',\n 'Srour',\n 'Shadid',\n 'Hajjar',\n 'Hanania',\n 'Kalb',\n 'Shadid',\n 'Bazzi',\n 'Mustafa',\n 'Masih',\n 'Ghanem',\n 'Haddad',\n 'Isa',\n 'Antoun',\n 'Sarraf',\n 'Sleiman',\n 'Dagher',\n 'Najjar',\n 'Malouf',\n 'Nahas',\n 'Naser',\n 'Saliba',\n 'Shamon',\n 'Malouf',\n 'Kalb',\n 'Daher',\n 'Maalouf',\n 'Wasem',\n 'Kanaan',\n 'Naifeh',\n 'Boutros',\n 'Moghadam',\n 'Masih',\n 'Sleiman',\n 'Aswad',\n 'Cham',\n 'Assaf',\n 'Quraishi',\n 'Shalhoub',\n 'Sabbag',\n 'Mifsud',\n 'Gaber',\n 'Shammas',\n 'Tannous',\n 'Sleiman',\n 'Bazzi',\n 'Quraishi',\n 'Rahal',\n 'Cham',\n 'Ghanem',\n 'Ghanem',\n 'Naser',\n 'Baba',\n 'Shamon',\n 'Almasi',\n 'Basara',\n 'Quraishi',\n 'Bata',\n 'Wasem',\n 'Shamoun',\n 'Deeb',\n 'Touma',\n 'Asfour',\n 'Deeb',\n 'Hadad',\n 'Naifeh',\n 'Touma',\n 'Bazzi',\n 'Shamoun',\n 'Nahas',\n 'Haddad',\n 'Arian',\n 'Kouri',\n 'Deeb',\n 'Toma',\n 'Halabi',\n 'Nazari',\n 'Saliba',\n 'Fakhoury',\n 'Hadad',\n 'Baba',\n 'Mansour',\n 'Sayegh',\n 'Antar',\n 'Deeb',\n 'Morcos',\n 'Shalhoub',\n 'Sarraf',\n 'Amari',\n 'Wasem',\n 'Ganim',\n 'Tuma',\n 'Fakhoury',\n 'Hadad',\n 'Hakimi',\n 'Nader',\n 'Said',\n 'Ganim',\n 'Daher',\n 'Ganem',\n 'Tuma',\n 'Boutros',\n 'Aswad',\n 'Sarkis',\n 'Daher',\n 'Toma',\n 'Boutros',\n 'Kanaan',\n 'Antar',\n 'Gerges',\n 'Kouri',\n 'Maroun',\n 'Wasem',\n 'Dagher',\n 'Naifeh',\n 'Bishara',\n 'Ba',\n 'Cham',\n 'Kalb',\n 'Bazzi',\n 'Bitar',\n 'Hadad',\n 'Moghadam',\n 'Sleiman',\n 'Shamoun',\n 'Antar',\n 'Atiyeh',\n 'Koury',\n 'Nahas',\n 'Kouri',\n 'Maroun',\n 'Nassar',\n 'Sayegh',\n 'Haik',\n 'Ghanem',\n 'Sayegh',\n 'Salib',\n 'Cham',\n 'Bata',\n 'Touma',\n 'Antoun',\n 'Antar',\n 'Bata',\n 'Botros',\n 'Shammas',\n 'Ganim',\n 'Sleiman',\n 'Seif',\n 'Moghadam',\n 'Ba',\n 'Tannous',\n 'Bazzi',\n 'Seif',\n 'Salib',\n 'Hadad',\n 'Quraishi',\n 'Halabi',\n 'Essa',\n 'Bahar',\n 'Kattan',\n 'Boutros',\n 'Nahas',\n 'Sabbagh',\n 'Kanaan',\n 'Sayegh',\n 'Said',\n 'Botros',\n 'Najjar',\n 'Toma',\n 'Bata',\n 'Atiyeh',\n 'Halabi',\n 'Tannous',\n 'Kouri',\n 'Shamoon',\n 'Kassis',\n 'Haddad',\n 'Tuma',\n 'Mansour',\n 'Antar',\n 'Kassis',\n 'Kalb',\n 'Basara',\n 'Rahal',\n 'Mansour',\n 'Handal',\n 'Morcos',\n 'Fakhoury',\n 'Hadad',\n 'Morcos',\n 'Kouri',\n 'Quraishi',\n 'Almasi',\n 'Awad',\n 'Naifeh',\n 'Koury',\n 'Asker',\n 'Maroun',\n 'Fakhoury',\n 'Sabbag',\n 'Sarraf',\n 'Shamon',\n 'Assaf',\n 'Boutros',\n 'Malouf',\n 'Nassar',\n 'Qureshi',\n 'Ghanem',\n 'Srour',\n 'Almasi',\n 'Qureshi',\n 'Ghannam',\n 'Mustafa',\n 'Najjar',\n 'Kassab',\n 'Shadid',\n 'Shamoon',\n 'Morcos',\n 'Atiyeh',\n 'Isa',\n 'Ba',\n 'Baz',\n 'Asker',\n 'Seif',\n 'Asghar',\n 'Hajjar',\n 'Deeb',\n 'Essa',\n 'Qureshi',\n 'Abboud',\n 'Ganem',\n 'Haddad',\n 'Koury',\n 'Nassar',\n 'Abadi',\n 'Toma',\n 'Tannous',\n 'Harb',\n 'Issa',\n 'Khouri',\n 'Mifsud',\n 'Kalb',\n 'Gaber',\n 'Ganim',\n 'Boulos',\n 'Samaha',\n 'Haddad',\n 'Sabbag',\n 'Wasem',\n 'Dagher',\n 'Rahal',\n 'Atiyeh',\n 'Antar',\n 'Asghar',\n 'Mansour',\n 'Awad',\n 'Boulos',\n 'Sarraf',\n 'Deeb',\n 'Abadi',\n 'Nazari',\n 'Daher',\n 'Gerges',\n 'Shamoon',\n 'Gaber',\n 'Amari',\n 'Sarraf',\n 'Nazari',\n 'Saliba',\n 'Naifeh',\n 'Nazari',\n 'Hakimi',\n 'Shamon',\n 'Abboud',\n 'Quraishi',\n 'Tahan',\n 'Safar',\n 'Hajjar',\n 'Srour',\n 'Gaber',\n 'Shalhoub',\n 'Attia',\n 'Safar',\n 'Said',\n 'Ganem',\n 'Nader',\n 'Asghar',\n 'Mustafa',\n 'Said',\n 'Antar',\n 'Botros',\n 'Nader',\n 'Ghannam',\n 'Asfour',\n 'Tahan',\n 'Mansour',\n 'Attia',\n 'Touma',\n 'Najjar',\n 'Kassis',\n 'Abboud',\n 'Bishara',\n 'Bazzi',\n 'Shalhoub',\n 'Shalhoub',\n 'Safar',\n 'Khoury',\n 'Nazari',\n 'Sabbag',\n 'Sleiman',\n 'Atiyeh',\n 'Kouri',\n 'Bitar',\n 'Zogby',\n 'Ghanem',\n 'Assaf',\n 'Abadi',\n 'Arian',\n 'Shalhoub',\n 'Khoury',\n 'Morcos',\n 'Shamon',\n 'Wasem',\n 'Abadi',\n 'Antoun',\n 'Baz',\n 'Naser',\n 'Assaf',\n 'Saliba',\n 'Nader',\n 'Mikhail',\n 'Naser',\n 'Daher',\n 'Morcos',\n 'Awad',\n 'Nahas',\n 'Sarkis',\n 'Malouf',\n 'Mustafa',\n 'Fakhoury',\n 'Ghannam',\n 'Shadid',\n 'Gaber',\n 'Koury',\n 'Atiyeh',\n 'Shamon',\n 'Boutros',\n 'Sarraf',\n 'Arian',\n 'Fakhoury',\n 'Abadi',\n 'Kassab',\n 'Nahas',\n 'Quraishi',\n 'Mansour',\n 'Samaha',\n 'Wasem',\n 'Seif',\n 'Fakhoury',\n 'Saliba',\n 'Cham',\n 'Bahar',\n 'Shamoun',\n 'Essa',\n 'Shamon',\n 'Asfour',\n 'Bitar',\n 'Cham',\n 'Tahan',\n 'Tannous',\n 'Daher',\n 'Khoury',\n 'Shamon',\n 'Bahar',\n 'Quraishi',\n 'Ghannam',\n 'Kassab',\n 'Zogby',\n 'Basara',\n 'Shammas',\n 'Arian',\n 'Sayegh',\n 'Naifeh',\n 'Mifsud',\n 'Sleiman',\n 'Arian',\n 'Kassis',\n 'Shamoun',\n 'Kassis',\n 'Harb',\n 'Mustafa',\n 'Boulos',\n 'Asghar',\n 'Shamon',\n 'Kanaan',\n 'Atiyeh',\n 'Kassab',\n 'Tahan',\n 'Bazzi',\n 'Kassis',\n 'Qureshi',\n 'Basara',\n 'Shalhoub',\n 'Sayegh',\n 'Haik',\n 'Attia',\n 'Maroun',\n 'Kassis',\n 'Sarkis',\n 'Harb',\n 'Assaf',\n 'Kattan',\n 'Antar',\n 'Sleiman',\n 'Touma',\n 'Sarraf',\n 'Bazzi',\n 'Boulos',\n 'Baz',\n 'Issa',\n 'Shamon',\n 'Shadid',\n 'Deeb',\n 'Sabbag',\n 'Wasem',\n 'Awad',\n 'Mansour',\n 'Saliba',\n 'Fakhoury',\n 'Arian',\n 'Bishara',\n 'Dagher',\n 'Bishara',\n 'Koury',\n 'Fakhoury',\n 'Naser',\n 'Nader',\n 'Antar',\n 'Gerges',\n 'Handal',\n 'Hanania',\n 'Shadid',\n 'Gerges',\n 'Kassis',\n 'Essa',\n 'Assaf',\n 'Shadid',\n 'Seif',\n 'Shalhoub',\n 'Shamoun',\n 'Hajjar',\n 'Baba',\n 'Sayegh',\n 'Mustafa',\n 'Sabbagh',\n 'Isa',\n 'Najjar',\n 'Tannous',\n 'Hanania',\n 'Ganem',\n 'Gerges',\n 'Fakhoury',\n 'Mifsud',\n 'Nahas',\n 'Bishara',\n 'Bishara',\n 'Abadi',\n 'Sarkis',\n 'Masih',\n 'Isa',\n 'Attia',\n 'Kalb',\n 'Essa',\n 'Boulos',\n 'Basara',\n 'Halabi',\n 'Halabi',\n 'Dagher',\n 'Attia',\n 'Kassis',\n 'Tuma',\n 'Gerges',\n 'Ghannam',\n 'Toma',\n 'Baz',\n 'Asghar',\n 'Zogby',\n 'Aswad',\n 'Hadad',\n 'Dagher',\n 'Naser',\n 'Shadid',\n 'Atiyeh',\n 'Zogby',\n 'Abboud',\n 'Tannous',\n 'Khouri',\n 'Atiyeh',\n 'Ganem',\n 'Maalouf',\n 'Isa',\n 'Maroun',\n 'Issa',\n 'Khouri',\n 'Harb',\n 'Nader',\n 'Awad',\n 'Nahas',\n 'Said',\n 'Baba',\n 'Totah',\n 'Ganim',\n 'Handal',\n 'Mansour',\n 'Basara',\n 'Malouf',\n 'Said',\n 'Botros',\n 'Samaha',\n 'Safar',\n 'Tahan',\n 'Botros',\n 'Shamoun',\n 'Handal',\n 'Sarraf',\n 'Malouf',\n 'Bishara',\n 'Aswad',\n 'Khouri',\n 'Baz',\n 'Asker',\n 'Toma',\n 'Koury',\n 'Gerges',\n 'Bishara',\n 'Boulos',\n 'Najjar',\n 'Aswad',\n 'Shamon',\n 'Kouri',\n 'Srour',\n 'Assaf',\n 'Tannous',\n 'Attia',\n 'Mustafa',\n 'Kattan',\n 'Asghar',\n 'Amari',\n 'Shadid',\n 'Said',\n 'Bazzi',\n 'Masih',\n 'Antar',\n 'Fakhoury',\n 'Shadid',\n 'Masih',\n 'Handal',\n 'Sarraf',\n 'Kassis',\n 'Salib',\n 'Hajjar',\n 'Totah',\n 'Koury',\n 'Totah',\n 'Mustafa',\n 'Sabbagh',\n 'Moghadam',\n 'Toma',\n 'Srour',\n 'Almasi',\n 'Totah',\n 'Maroun',\n 'Kattan',\n 'Naifeh',\n 'Sarkis',\n 'Mikhail',\n 'Nazari',\n 'Boutros',\n 'Guirguis',\n 'Gaber',\n 'Kassis',\n 'Masih',\n 'Hanania',\n 'Maloof',\n 'Quraishi',\n 'Cham',\n 'Hadad',\n 'Tahan',\n 'Bitar',\n 'Arian',\n 'Gaber',\n 'Baz',\n 'Mansour',\n 'Kalb',\n 'Sarkis',\n 'Attia',\n 'Antar',\n 'Asfour',\n 'Said',\n 'Essa',\n 'Koury',\n 'Hadad',\n 'Tuma',\n 'Moghadam',\n 'Sabbagh',\n 'Amari',\n 'Dagher',\n 'Srour',\n 'Antoun',\n 'Sleiman',\n 'Maroun',\n 'Tuma',\n 'Nahas',\n 'Hanania',\n 'Sayegh',\n 'Amari',\n 'Sabbagh',\n 'Said',\n 'Cham',\n 'Asker',\n 'Nassar',\n 'Bitar',\n 'Said',\n 'Dagher',\n 'Safar',\n 'Khouri',\n 'Totah',\n 'Khoury',\n 'Salib',\n 'Basara',\n 'Abboud',\n 'Baz',\n 'Isa',\n 'Cham',\n 'Amari',\n 'Mifsud',\n 'Hadad',\n 'Rahal',\n 'Khoury',\n 'Bazzi',\n 'Basara',\n 'Totah',\n 'Ghannam',\n 'Koury',\n 'Malouf',\n 'Zogby',\n 'Zogby',\n 'Boutros',\n 'Nassar',\n 'Handal',\n 'Hajjar',\n 'Maloof',\n 'Abadi',\n 'Maroun',\n 'Mifsud',\n 'Kalb',\n 'Amari',\n 'Hakimi',\n 'Boutros',\n 'Masih',\n 'Kattan',\n 'Haddad',\n 'Arian',\n 'Nazari',\n 'Assaf',\n 'Attia',\n 'Wasem',\n 'Gerges',\n 'Asker',\n 'Tahan',\n 'Fakhoury',\n 'Shadid',\n 'Sarraf',\n 'Attia',\n 'Naifeh',\n 'Aswad',\n 'Deeb',\n 'Tannous',\n 'Totah',\n 'Cham',\n 'Baba',\n 'Najjar',\n 'Hajjar',\n 'Shamoon',\n 'Handal',\n 'Awad',\n 'Guirguis',\n 'Awad',\n 'Ganem',\n 'Naifeh',\n 'Khoury',\n 'Hajjar',\n 'Moghadam',\n 'Mikhail',\n 'Ghannam',\n 'Guirguis',\n 'Tannous',\n 'Kanaan',\n 'Handal',\n 'Khoury',\n 'Kalb',\n 'Qureshi',\n 'Najjar',\n 'Atiyeh',\n 'Gerges',\n 'Nassar',\n 'Tahan',\n 'Hadad',\n 'Fakhoury',\n 'Salib',\n 'Wasem',\n 'Bitar',\n 'Fakhoury',\n 'Attia',\n 'Awad',\n 'Totah',\n 'Deeb',\n 'Touma',\n 'Botros',\n 'Nazari',\n 'Nahas',\n 'Kouri',\n 'Ghannam',\n 'Assaf',\n 'Asfour',\n 'Sarraf',\n 'Naifeh',\n 'Toma',\n 'Asghar',\n 'Abboud',\n 'Issa',\n 'Sabbag',\n 'Sabbagh',\n 'Isa',\n 'Koury',\n 'Kattan',\n 'Shamoon',\n 'Rahal',\n 'Kalb',\n 'Naser',\n 'Masih',\n 'Sayegh',\n 'Dagher',\n 'Asker',\n 'Maroun',\n 'Dagher',\n 'Sleiman',\n 'Botros',\n 'Sleiman',\n 'Harb',\n 'Tahan',\n 'Tuma',\n 'Said',\n 'Hadad',\n 'Samaha',\n 'Harb',\n 'Cham',\n 'Atiyeh',\n 'Haik',\n 'Malouf',\n 'Bazzi',\n 'Harb',\n 'Malouf',\n 'Ghanem',\n 'Cham',\n 'Asghar',\n 'Samaha',\n 'Khouri',\n 'Nassar',\n 'Rahal',\n 'Baz',\n 'Kalb',\n 'Rahal',\n 'Gerges',\n 'Cham',\n 'Sayegh',\n 'Shadid',\n 'Morcos',\n 'Shamoon',\n 'Hakimi',\n 'Shamoon',\n 'Qureshi',\n 'Ganim',\n 'Shadid',\n 'Khoury',\n 'Boutros',\n 'Hanania',\n 'Antoun',\n 'Naifeh',\n 'Deeb',\n 'Samaha',\n 'Awad',\n 'Asghar',\n 'Awad',\n 'Saliba',\n 'Shamoun',\n 'Mikhail',\n 'Hakimi',\n 'Mikhail',\n 'Cham',\n 'Halabi',\n 'Sarkis',\n 'Kattan',\n 'Nazari',\n 'Safar',\n 'Morcos',\n 'Khoury',\n 'Essa',\n 'Nassar',\n 'Haik',\n 'Shadid',\n 'Fakhoury',\n 'Najjar',\n 'Arian',\n 'Botros',\n 'Daher',\n 'Saliba',\n 'Saliba',\n 'Kattan',\n 'Hajjar',\n 'Nader',\n 'Daher',\n 'Nassar',\n 'Maroun',\n 'Harb',\n 'Nassar',\n 'Antar',\n 'Shammas',\n 'Toma',\n 'Antar',\n 'Koury',\n 'Nader',\n 'Botros',\n 'Bahar',\n 'Najjar',\n 'Maloof',\n 'Salib',\n 'Malouf',\n 'Mansour',\n 'Bazzi',\n 'Atiyeh',\n 'Kanaan',\n 'Bishara',\n 'Hakimi',\n 'Saliba',\n 'Tuma',\n 'Mifsud',\n 'Hakimi',\n 'Assaf',\n 'Nassar',\n 'Sarkis',\n 'Bitar',\n 'Isa',\n 'Halabi',\n 'Shamon',\n 'Qureshi',\n 'Bishara',\n 'Maalouf',\n 'Srour',\n 'Boulos',\n 'Safar',\n 'Shamoun',\n 'Ganim',\n 'Abadi',\n 'Koury',\n 'Shadid',\n 'Zogby',\n 'Boutros',\n 'Shadid',\n 'Hakimi',\n 'Bazzi',\n 'Isa',\n 'Totah',\n 'Salib',\n 'Shamoon',\n 'Gaber',\n 'Antar',\n 'Antar',\n 'Najjar',\n 'Fakhoury',\n 'Malouf',\n 'Salib',\n 'Rahal',\n 'Boulos',\n 'Attia',\n 'Said',\n 'Kassis',\n 'Bahar',\n 'Bazzi',\n 'Srour',\n 'Antar',\n 'Nahas',\n 'Kassis',\n 'Samaha',\n 'Quraishi',\n 'Asghar',\n 'Asker',\n 'Antar',\n 'Totah',\n 'Haddad',\n 'Maloof',\n 'Kouri',\n 'Basara',\n 'Bata',\n 'Antar',\n 'Shammas',\n 'Arian',\n 'Gerges',\n 'Seif',\n 'Almasi',\n 'Tuma',\n 'Shamoon',\n 'Khoury',\n 'Hakimi',\n 'Abboud',\n 'Baz',\n 'Seif',\n 'Issa',\n 'Nazari',\n 'Harb',\n 'Shammas',\n 'Amari',\n 'Totah',\n 'Malouf',\n 'Sarkis',\n 'Naser',\n 'Zogby',\n 'Handal',\n 'Naifeh',\n 'Cham',\n 'Hadad',\n 'Gerges',\n 'Kalb',\n 'Shalhoub',\n 'Saliba',\n 'Tannous',\n 'Tahan',\n 'Tannous',\n 'Kassis',\n 'Shadid',\n 'Sabbag',\n 'Tahan',\n 'Abboud',\n 'Nahas',\n 'Shamoun',\n ...]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catToLine['Arabic']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-04T13:12:09.761119550Z",
     "start_time": "2023-12-04T13:12:09.696211306Z"
    }
   },
   "id": "7afc162e70bdb941"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-04T13:11:44.887717557Z"
    }
   },
   "id": "96773d75880d553"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
