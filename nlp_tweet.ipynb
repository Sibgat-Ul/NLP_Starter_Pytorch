{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:57.922905519Z",
     "start_time": "2023-12-03T05:13:57.501589220Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_validate, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('../dsets/nlp_tweet/train.csv', index_col='id')\n",
    "test = pd.read_csv('../dsets/nlp_tweet/test.csv', index_col='id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:57.989518242Z",
     "start_time": "2023-12-03T05:13:57.963173462Z"
    }
   },
   "id": "69d65a46dbc4edb3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 4) (3263, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:57.990067429Z",
     "start_time": "2023-12-03T05:13:57.963461430Z"
    }
   },
   "id": "bffa1dd80c42046e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "keyword_train = train.keyword.unique()[1:]\n",
    "keyword_test = test.keyword.unique()[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:57.990255688Z",
     "start_time": "2023-12-03T05:13:57.963750009Z"
    }
   },
   "id": "544747ccc9cb5a60"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "keyword_train = list(map(lambda x: x.replace('%20', ' '), keyword_train))\n",
    "keyword_test = list(map(lambda x: x.replace('%20', ' '), keyword_test))\n",
    "keywords = pd.DataFrame({'train': keyword_train, 'test': keyword_test})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:57.990387688Z",
     "start_time": "2023-12-03T05:13:57.963983707Z"
    }
   },
   "id": "5e0d96eae915f430"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train.drop(['keyword', 'location'], inplace=True, axis=1)\n",
    "test.drop(['keyword', 'location'], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:57.990578857Z",
     "start_time": "2023-12-03T05:13:57.969108432Z"
    }
   },
   "id": "7ff6e50760af0f8f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    to_rem = re.compile(r'https?://\\S+|www\\.\\S+|[^A-Za-z0-9]|\\s+')\n",
    "    rem_space = re.compile(r'\\s{2,}')\n",
    "    return rem_space.sub(' ', to_rem.sub(' ', text))\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    new_sentence_word = list()\n",
    "\n",
    "    for word in words:\n",
    "        new_word = wnl.lemmatize(word, wordnet.VERB)\n",
    "        new_sentence_word.append(new_word)\n",
    "\n",
    "    new_sentence = ' '.join(new_sentence_word)\n",
    "    new_sentence = new_sentence.strip()\n",
    "\n",
    "    return new_sentence\n",
    "\n",
    "def get_iterator(dataset, batch_size, train=True, shuffle=True, repeat=False):\n",
    "    dataset = Make_Dataset(dataset)\n",
    "    dataset_iter = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataset_iter\n",
    "\n",
    "def prepare_csv(train, test, seed=42, ratio=0.2):\n",
    "    idx = np.arange(train.shape[0])\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    val_size = int(len(idx)*ratio)\n",
    "\n",
    "    train.iloc[idx[val_size:], :][['target', 'text']].to_csv('./prep_train.csv')\n",
    "\n",
    "    train.iloc[idx[:val_size], :][['target', 'text']].to_csv('./prep_val.csv')\n",
    "\n",
    "    test.to_csv('./test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:57.991912970Z",
     "start_time": "2023-12-03T05:13:57.981635151Z"
    }
   },
   "id": "bf34e7e10918c7ba"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: normalize_text(x))\n",
    "test['text'] = test['text'].apply(lambda x: normalize_text(x))\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: lemmatize_sentence(x))\n",
    "test['text'] = test['text'].apply(lambda x: lemmatize_sentence(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:13:59.309997404Z",
     "start_time": "2023-12-03T05:13:57.990129749Z"
    }
   },
   "id": "a7c06043ee33d854"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "    \n",
    "from torchtext.vocab import Vectors, GloVe, Vocab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.002249088Z",
     "start_time": "2023-12-03T05:13:59.311044899Z"
    }
   },
   "id": "57dcfd16f25fe1c2"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Make_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, target_transform=None):\n",
    "        self.data = csv_file\n",
    "        self.text_data = csv_file['text']\n",
    "        self.target = csv_file['target']\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.data['text'].iloc[item], self.data['target'].iloc[item]\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.057824415Z",
     "start_time": "2023-12-03T05:14:01.005407093Z"
    }
   },
   "id": "25cf283a52ec596"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    text  target\nid                                                              \n1      our deeds be the reason of this earthquake may...       1\n4                  forest fire near la ronge sask canada       1\n5      all residents ask to shelter in place be be no...       1\n6      13 000 people receive wildfires evacuation ord...       1\n7      just get send this photo from ruby alaska as s...       1\n...                                                  ...     ...\n10869  two giant crane hold a bridge collapse into ne...       1\n10870  aria ahrary thetawniest the out of control wil...       1\n10871            m1 94 01 04 utc 5km s of volcano hawaii       1\n10872  police investigate after an e bike collide wit...       1\n10873  the latest more home raze by northern californ...       1\n\n[7613 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>our deeds be the reason of this earthquake may...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>forest fire near la ronge sask canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>all residents ask to shelter in place be be no...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>13 000 people receive wildfires evacuation ord...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>just get send this photo from ruby alaska as s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10869</th>\n      <td>two giant crane hold a bridge collapse into ne...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10870</th>\n      <td>aria ahrary thetawniest the out of control wil...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10871</th>\n      <td>m1 94 01 04 utc 5km s of volcano hawaii</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10872</th>\n      <td>police investigate after an e bike collide wit...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10873</th>\n      <td>the latest more home raze by northern californ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.061416798Z",
     "start_time": "2023-12-03T05:14:01.007386633Z"
    }
   },
   "id": "7e4ffdd1d06a4e22"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "words = []\n",
    "stop_word = set(stopwords.words('english'))\n",
    "\n",
    "for text in train['text']:\n",
    "    tokenized = word_tokenize(text)\n",
    "    for i in tokenized:\n",
    "        if i not in stop_word and i not in ['1','2','3','4','5','6','7','8','9','0']:\n",
    "            words.append(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.471317854Z",
     "start_time": "2023-12-03T05:14:01.021069216Z"
    }
   },
   "id": "8f452177fc4fa912"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "corpus = Counter(words)\n",
    "corpus_ = sorted(corpus,key=corpus.get,reverse=True)\n",
    "ohe = {w:i+1 for i,w in enumerate(corpus_)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.471739332Z",
     "start_time": "2023-12-03T05:14:01.441715029Z"
    }
   },
   "id": "25c8755f3557ebf6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "final_list_train = []\n",
    "final_list_test = []\n",
    "\n",
    "for sent in train['text']:\n",
    "    final_list_train.append([ohe[word] for word in sent.split() if word in ohe.keys()])\n",
    "\n",
    "for sent in test['text']:\n",
    "    final_list_test.append([ohe[word] for word in sent.split() if word in ohe.keys()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.513526046Z",
     "start_time": "2023-12-03T05:14:01.444399386Z"
    }
   },
   "id": "fb2510e27f0db273"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.514010404Z",
     "start_time": "2023-12-03T05:14:01.481537193Z"
    }
   },
   "id": "94347f987e0cc592"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.514291582Z",
     "start_time": "2023-12-03T05:14:01.483271505Z"
    }
   },
   "id": "909179390a02759a"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    for i, text in enumerate(sentences):\n",
    "        \n",
    "        features = np.zeros(seq_len,dtype=int)\n",
    "        for j, t in enumerate(text):\n",
    "            features[j] = t\n",
    "        sentences[i] = features\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.514576361Z",
     "start_time": "2023-12-03T05:14:01.484930147Z"
    }
   },
   "id": "15cdafd4d9ef9480"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "padded_train = padding_(final_list_train, 200)\n",
    "padded_test = padding_(final_list_test, 200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.605856562Z",
     "start_time": "2023-12-03T05:14:01.487908272Z"
    }
   },
   "id": "fe3d8c5a9af788a8"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "Xtrain_tensor = torch.LongTensor(padded_train)\n",
    "Ytrain_tensor = torch.LongTensor(np.array(train['target']))\n",
    "\n",
    "val_x = Xtrain_tensor[-13:]\n",
    "val_y = Ytrain_tensor[-13:]\n",
    "\n",
    "Xtrain_tensor = Xtrain_tensor[:-13]\n",
    "Ytrain_tensor = Ytrain_tensor[:-13]\n",
    "\n",
    "test_tensor = torch.LongTensor(padded_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.749555616Z",
     "start_time": "2023-12-03T05:14:01.747864345Z"
    }
   },
   "id": "8c7a02110a8577a4"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "7600"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.818132879Z",
     "start_time": "2023-12-03T05:14:01.749729376Z"
    }
   },
   "id": "759d668608d29158"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_loader = DataLoader(TensorDataset(Xtrain_tensor, Ytrain_tensor), batch_size=50, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_tensor), batch_size=1, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(Xtrain_tensor, Ytrain_tensor), batch_size=50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.839811463Z",
     "start_time": "2023-12-03T05:14:01.764565403Z"
    }
   },
   "id": "907cd97099881d0b"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.864077354Z",
     "start_time": "2023-12-03T05:14:01.774229245Z"
    }
   },
   "id": "e32132fab6cd671"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,output_dim,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                            num_layers=no_layers, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:01.864344752Z",
     "start_time": "2023-12-03T05:14:01.778030616Z"
    }
   },
   "id": "a097c6fce8cb5b63"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(14894, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = 2\n",
    "vocab_size = len(ohe)+1\n",
    "embedding_dim = 256\n",
    "output_dim = 2\n",
    "\n",
    "model = SentimentRNN(layers, vocab_size, output_dim, 512, embedding_dim)\n",
    "model.to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:02.531055566Z",
     "start_time": "2023-12-03T05:14:01.784597104Z"
    }
   },
   "id": "1476858e59e13b61"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:14:02.728034168Z",
     "start_time": "2023-12-03T05:14:02.496001468Z"
    }
   },
   "id": "3330e42ff7024e8d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss : 0.6833026577767572 val_loss : 0.6831850876149378\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "Validation loss decreased (inf --> 0.683185).  Saving model ...\n",
      "==================================================\n",
      "Epoch 2\n",
      "train_loss : 0.6833765871430698 val_loss : 0.6837193111055776\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 3\n",
      "train_loss : 0.6834538790740465 val_loss : 0.6833664719995699\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 4\n",
      "train_loss : 0.6833884025874891 val_loss : 0.6829421837863169\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "Validation loss decreased (0.683185 --> 0.682942).  Saving model ...\n",
      "==================================================\n",
      "Epoch 5\n",
      "train_loss : 0.6832401823056372 val_loss : 0.6830517759448603\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 6\n",
      "train_loss : 0.6833346352765435 val_loss : 0.6831266648675266\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 7\n",
      "train_loss : 0.6831200852205879 val_loss : 0.6829424774960468\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 8\n",
      "train_loss : 0.6834887426934744 val_loss : 0.6829568269221407\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 9\n",
      "train_loss : 0.6830184843979383 val_loss : 0.6830647936777064\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 10\n",
      "train_loss : 0.6833563117604506 val_loss : 0.6831208279258326\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 11\n",
      "train_loss : 0.6831971195183302 val_loss : 0.6829592373810316\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 12\n",
      "train_loss : 0.6833461558348254 val_loss : 0.6829660962286749\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 13\n",
      "train_loss : 0.6832877605369216 val_loss : 0.6829535984679272\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 14\n",
      "train_loss : 0.6833813959046414 val_loss : 0.6829426966999707\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 15\n",
      "train_loss : 0.6831912414023751 val_loss : 0.6834108311879007\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 16\n",
      "train_loss : 0.683441197009463 val_loss : 0.6832914219090813\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 17\n",
      "train_loss : 0.6832021835603213 val_loss : 0.6831411660501832\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 18\n",
      "train_loss : 0.6831194256481371 val_loss : 0.6830578560107633\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n",
      "Epoch 19\n",
      "train_loss : 0.6833865160220548 val_loss : 0.6829408281727841\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "Validation loss decreased (0.682942 --> 0.682941).  Saving model ...\n",
      "==================================================\n",
      "Epoch 20\n",
      "train_loss : 0.6831842143284647 val_loss : 0.6829790771007538\n",
      "train_accuracy : 57.131578947368425 val_accuracy : 57.131578947368425\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "clip = 5\n",
    "epochs = 20\n",
    "valid_loss_min = np.Inf\n",
    "batch_size = 50\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        output, val_h = model(inputs, val_h)\n",
    "        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        accuracy = acc(output,labels)\n",
    "        val_acc += accuracy\n",
    "\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), 'state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:18:09.972614047Z",
     "start_time": "2023-12-03T05:15:22.204015580Z"
    }
   },
   "id": "1002ee3f976d585c"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "\n",
    "def predict(padded):\n",
    "    inputs = padded.to(device)\n",
    "    batch_size = 1\n",
    "    h = model.init_hidden(batch_size)\n",
    "    h = tuple([each.data for each in h])\n",
    "    output, h = model(inputs, h)\n",
    "    print(output.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:36.830826414Z",
     "start_time": "2023-12-03T05:26:36.823117502Z"
    }
   },
   "id": "b9e6168edd04a00b"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43303418159484863\n"
     ]
    }
   ],
   "source": [
    "y = predict(test_tensor[6:7])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T05:26:50.571792229Z",
     "start_time": "2023-12-03T05:26:50.555510180Z"
    }
   },
   "id": "67ca31ab63ef00a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_test[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-03T05:15:03.946416185Z"
    }
   },
   "id": "3cf5f176e68d2f8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-03T05:15:03.946493404Z"
    }
   },
   "id": "eebefe92dd486e5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
